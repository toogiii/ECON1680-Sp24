{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bae82a9-d828-4778-866f-bb33846c12db",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "## Econ 1680: MLTA and Econ\n",
    "\n",
    "#### Name:\n",
    "\n",
    "In this course we will be using Python for writing code to apply machine learning and text analysis methods to economics topics. Python is free, flexible, offers a variety of predefined packages, and is popular. It can handle everything from the statistical analysis of Stata to the matrix algebra and simulation of Matlab.\n",
    "\n",
    "This assignment is meant to introduce you to how we will be using Python in this course. For this assignment, you should write/type your answers into this worksheet. You may discuss the problem set with your class mates, but every student must do their own work. \n",
    "\n",
    "It is always important to cite our references that help us in our work. Please list the students you work with here:\n",
    "\n",
    "1\\. \n",
    "\n",
    "2\\.\n",
    "\n",
    "3\\.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a1be8a",
   "metadata": {},
   "source": [
    "## I. PRELIMINARIES \n",
    "\n",
    "Preliminaries are listed in the HW1 Assignment on Canvas. It includes the following:\n",
    "\n",
    "* Downloading and installing Python/Anaconda\n",
    "\n",
    "* Installing necessary pacakges for the homework assignment\n",
    "\n",
    "* Setting up your GitHub account for keeping track of your work\n",
    "\n",
    "* How to sumbit your homework assignment and code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c14bd6-773e-48ef-b166-5f66fa375b89",
   "metadata": {},
   "source": [
    "## II.\tNUMERICAL DATA \n",
    "\n",
    "### 1. Zillow Data\n",
    "\n",
    "Access Zillow Real Estate Data using the Nasdaq Data Link API. Nasdaq Data Link is a dataset aggregation website that also has other economics datasets. These types of websites can make it easier to get data and to explore what types of datasets are available. \n",
    "    \n",
    "- Set up free account with Nasdaq Data Link (https://data.nasdaq.com/). Find your API Key in your Account Settings. You will need this to download the data.\n",
    "\n",
    "- Find the “Zillow Real Estate Data” that is Free (https://data.nasdaq.com/databases/ZILLOW/data) This will be the data you will download.\n",
    "\n",
    "- Click on the “Usage” tab, then select the “Python” sub-tab for instructions on using the Nasdaq Data Link API.\n",
    "\n",
    "- To decide which variables and regions we want to download data for, we will first download information on the indicators and regions. In a python environment, you will run the code above to import packages, setup your API connection, and download the indicator and region dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "033393dc-a223-4d56-a609-d18f0c64a413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nasdaqdatalink\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Change the API key to yours\n",
    "nasdaqdatalink.ApiConfig.api_key = 'WEbwnsLprrT9VWcqjbVg'\n",
    "df_zillow_indicators = nasdaqdatalink.get_table('ZILLOW/INDICATORS', paginate=True)\n",
    "df_zillow_regions = nasdaqdatalink.get_table('ZILLOW/REGIONS', paginate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a6581f-0f58-4fb4-8153-a0ca7605c633",
   "metadata": {},
   "source": [
    "#### 1a. What does ZHVI in the indicator descriptions stand for? \n",
    "\n",
    "Answer: ZHVI stands for Zillow Home Value Index (https://www.zillow.com/research/data/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66542de",
   "metadata": {},
   "source": [
    "#### 1b. What is the indicator, description, and category of row 38 in df_zillow_indicators? \n",
    "\n",
    "Hint: use `.iloc[]` \n",
    "\n",
    "Answer: The indicator is LRAW, with description Median List Price (Raw, All Homes, Weekly View). The category is Inventory and Sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "694011c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     indicator_id                                          indicator  \\\n",
      "None                                                                   \n",
      "0            ZSFH           ZHVI Single-Family Homes Time Series ($)   \n",
      "1            ZCON                   ZHVI Condo/Co-op Time Series ($)   \n",
      "2            ZATT           ZHVI All Homes- Top Tier Time Series ($)   \n",
      "3            ZALL  ZHVI All Homes (SFR, Condo/Co-op) Time Series ($)   \n",
      "4            ZABT        ZHVI All Homes- Bottom Tier Time Series ($)   \n",
      "5            Z5BR                    ZHVI 5+ Bedroom Time Series ($)   \n",
      "6            Z4BR                     ZHVI 4-Bedroom Time Series ($)   \n",
      "7            Z3BR                     ZHVI 3-Bedroom Time Series ($)   \n",
      "8            Z2BR                     ZHVI 2-Bedroom Time Series ($)   \n",
      "9            Z1BR                     ZHVI 1-Bedroom Time Series ($)   \n",
      "\n",
      "         category  \n",
      "None               \n",
      "0     Home values  \n",
      "1     Home values  \n",
      "2     Home values  \n",
      "3     Home values  \n",
      "4     Home values  \n",
      "5     Home values  \n",
      "6     Home values  \n",
      "7     Home values  \n",
      "8     Home values  \n",
      "9     Home values  \n",
      "indicator_id                                               LRAW\n",
      "indicator       Median List Price (Raw, All Homes, Weekly View)\n",
      "category                                    Inventory and sales\n",
      "Name: 38, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Write the code you need (if any) to answer the questions above\n",
    "# Get all indicators containing 'ZHVI'\n",
    "print(df_zillow_indicators.loc[df_zillow_indicators['indicator'].str.contains('ZHVI')])\n",
    "\n",
    "# Get row 38 of INDICATORS\n",
    "print(df_zillow_indicators.iloc[38])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412310d5",
   "metadata": {},
   "source": [
    "#### 1c. In df_zillow_regions, how many regions are there when you search for “Providence;RI”? What is the region_id number for Providence, RI? \n",
    "\n",
    "Hint: use `.str.contains('Providence;RI')` \n",
    "\n",
    "Answer: There are 4 regions when we search for \"Providence;RI\". Providence, RI has a region_id number of 26637."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e093d76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      region_id region_type                                             region\n",
      "None                                                                          \n",
      "52365      4488        city  East Providence;RI;Providence-Warwick, RI-MA;P...\n",
      "57533     40069        city  North Providence;RI;Providence-Warwick, RI-MA;...\n",
      "72116    271242       neigh  Lower South Providence;RI;Providence-Warwick, ...\n",
      "74508     26637        city  Providence;RI;Providence-Warwick, RI-MA;Provid...\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# Write the code you need (if any) to answer the questions above\n",
    "# Get all regions containing 'Providence;RI'\n",
    "contains_prov = df_zillow_regions.loc[df_zillow_regions['region'].str.contains('Providence;RI')]\n",
    "print(contains_prov)\n",
    "print(len(contains_prov))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a5069c",
   "metadata": {},
   "source": [
    "#### 1d. Download a dataframe the city of Providence, RI on ZHVI Single-Family Home values with the correct indicator and region IDs entered using the following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44be6077-40cf-4e86-9a87-c1ed1cd67a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zillow_sfh = nasdaqdatalink.get_table('ZILLOW/DATA', indicator_id='ZSFH' , region_id='26637',paginate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebaa801-231f-48cb-b815-b82e3838c64d",
   "metadata": {},
   "source": [
    "### 2. Descriptive statistics \n",
    "\n",
    "#### 2a. What is the data frequency in df_zillow_sfh? \n",
    "\n",
    "Answer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc8f52a",
   "metadata": {},
   "source": [
    "#### 2b. What is the median dollar value of a home in df_zillow_sfh? \n",
    "\n",
    "Answer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11da2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code you need (if any) to answer the questions above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011a4691",
   "metadata": {},
   "source": [
    "#### 2c. What is the median dollar value of a home in df_zillow_sfh for the year of 2020? \n",
    "\n",
    "Hint: use  `[df_zillow_sfh['date'].dt.year==2020]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba99fd1-8807-4e72-8a2f-cf6f058e4a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code you need (if any) to answer the questions above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16c09f0-9ee6-430e-a370-ac6fa2f63b14",
   "metadata": {},
   "source": [
    "### 3. Visualize the Data\n",
    "\n",
    "#### 3a. Plot a time series graph for values df_zillow_sfh. Be sure to title your graph and label your axes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6885079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code you need (if any) to answer the questions above:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a90ad69",
   "metadata": {},
   "source": [
    "#### 3b. Plot time series graph for *yearly median* values df_zillow_sfh. Be sure to title your graph and label your axes. \n",
    "\n",
    "Hint: you will can create a new dataframe by creating a ‘year’ column using .dt.year and then use `.groupby(by=['year']).median(numeric_only=True)` to make a yearly dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a367b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code you need (if any) to answer the questions above:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ad8e24",
   "metadata": {},
   "source": [
    "#### 3c. What looks different in these graphs? Why?\n",
    "\n",
    "Answer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3ee237",
   "metadata": {},
   "source": [
    "#### 3d. Describe the patterns in the graph. What does it say about the housing market in Providence, RI over time? In recent years? What additional data would you need to make claims about what is changing this price? \n",
    "\n",
    "Answer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cf832c-39ba-4143-8790-2e89c72ee11d",
   "metadata": {},
   "source": [
    "## III\tTEXT DATA \n",
    "\n",
    "### 4. US News Data\n",
    "Download US Economic News Dataset from Kaggle.com: Sign up for a free account with Kaggle.com. This website hosts data science competitions and often has cool datasets available for download. We will be using the US Economic News Dataset at https://www.kaggle.com/heeraldedhia/us-economic-news-articles. Download the CSV file from the website by clicking “Download.”\n",
    "     \n",
    "Load a subset of the data into Jupyter/Spyder/Python: Sometimes you may be working with a large dataset and it is therefore important to understand how to load a subset of the data at a time. The US Economic News dataset has 8,000 observations.\n",
    "\n",
    "#### 4a. Run the code below and explain in words each of the lines of code with comments (use # to comment): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f6b60c-27bb-4247-95cd-9127a8779da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "#\n",
    "folder_path = 'TYPE FILE PATH TO WHERE YOU HAVE THE DATA'\n",
    "\n",
    "#\n",
    "fileReader = open(os.path.join(folder_path, \"US-Economic-News.csv\"), \"r\", encoding=\"unicode_escape\")\n",
    "csvReader = csv.reader(fileReader)\n",
    "\n",
    "#\n",
    "fileWriter = open(os.path.join(folder_path, \"Subset_US_Economic_News.csv\"), \"w\", encoding=\"unicode_escape\", newline='')\n",
    "csvWriter = csv.writer(fileWriter)\n",
    "\n",
    "#\n",
    "acHeader = next(csvReader)\n",
    "csvWriter.writerow(acHeader)\n",
    "\n",
    "#\n",
    "for index, acRow in enumerate(csvReader):\n",
    "    if index < 800:\n",
    "        csvWriter.writerow(acRow)\n",
    "\n",
    "#\n",
    "fileReader.close()\n",
    "fileWriter.close()\n",
    "\n",
    "#\n",
    "df_news = pd.read_csv(os.path.join(folder_path,\"Subset_US_Economic_News.csv\"), encoding='unicode_escape')\n",
    "df_news['date'] = pd.to_datetime(df_news['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25414d6",
   "metadata": {},
   "source": [
    "#### 4b.  What code would you write to keep only the ‘date’, ‘headline’, and ‘text’ columns in the dataframe? Write and run that code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fa68b4-5c8d-45a8-ad9a-79be7f725d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code you need (if any) for b.:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923e9110-f565-418d-b0d1-14713f54987b",
   "metadata": {},
   "source": [
    "### 5. String Extraction\n",
    "\n",
    "This dataframe is full of text data about US Economic News. When we try to extract information from text, formatting of words and string in code is very important.\n",
    "\n",
    "#### 5a. Count the number of headlines that have ‘US’ in them. \n",
    "\n",
    "Hint: loop over `df_news[‘headlines’]`. \n",
    "\n",
    "Answer: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd12f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code you need (if any) to answer the question above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797927bb",
   "metadata": {},
   "source": [
    "#### 5b. Count the number of headlines that have ‘us’ in them.\n",
    "\n",
    "Answer: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dc6d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code you need (if any) to answer the question above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad5a8f0",
   "metadata": {},
   "source": [
    "#### 5c. Why are these counts different? \n",
    "\n",
    "Hint: tell python to check if ‘us’ is in the string ‘trust’. Then tell python to check if ‘ us ‘ is in the string ‘trust’. \n",
    "\n",
    "Answer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57089a48-fb8d-4b1a-8d1b-e8fe02d0bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code you need (if any) to answer the question above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0412a30c-3bd1-473d-8c73-ca99abe8f634",
   "metadata": {},
   "source": [
    "### 6. Annotate Text Processing Code\n",
    "\n",
    "In text analysis, we will need to perform a few tasks to clean the data to prepare it for consistent analysis. \n",
    "\n",
    "#### 6a. Run the code and explain what each line does as comments (use # to comment): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88807593-27ec-433e-a98a-af5b3db35126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "#\n",
    "table_punctuation = str.maketrans('', '', '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~') \n",
    "\n",
    "#\n",
    "token_list = []\n",
    "for i, row in enumerate(df_news['text']):\n",
    "    text = row.translate(table_punctuation)\n",
    "    tokens = [word.lower() for word in nltk.tokenize.word_tokenize(text) if word.lower() not in stops]\n",
    "    token_list.append(tokens)\n",
    "\n",
    "#\n",
    "df_news['tokens'] = token_list\n",
    "\n",
    "#\n",
    "monetary_policy_wordlist = ['monetary', 'fed ', 'federal reserve', 'Federal Reserve', 'Monetary']\n",
    "\n",
    "#\n",
    "tally = 0\n",
    "monetary_text = []\n",
    "for row in df_news['text']:\n",
    "    mon = 0\n",
    "    if any(keyword in row for keyword in monetary_policy_wordlist):\n",
    "        tally += 1\n",
    "        mon = 1\n",
    "    monetary_text.append(mon)\n",
    "print(tally)\n",
    "df_news['monetary_flag'] = monetary_text\n",
    "\n",
    "#\n",
    "df_monetarynews = df_news[df_news['monetary_flag']==1]\n",
    "df_nonmonetarynews = df_news[df_news['monetary_flag']!=1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5fda93-8a92-4dc7-8d4d-a54d8ab7bd38",
   "metadata": {},
   "source": [
    "### 7. Compare Monetary News\n",
    "\n",
    "Compare and contrast the news articles about monetary policy in the US and those about non-monetary-policy economics in the US.\n",
    "\n",
    "#### 7a. Adapt the below code to answer the subsequent questions (i-iii)\n",
    "```python\n",
    "# This code calculates the top 30 most common words in df_news. \n",
    "from collections import Counter\n",
    "top_N = 30\n",
    "words = [word for tokenlist in df_news['tokens'].tolist() for word in tokenlist]\n",
    "topwords = pd.DataFrame(Counter(words).most_common(top_N),\n",
    "                           columns=['Word', 'Count']).set_index('Word')\n",
    "print(topwords)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23cfa6f",
   "metadata": {},
   "source": [
    "#### 7a.i. What are the 15 most common words from df_monetarynews?\n",
    "\n",
    "Answer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f74d0d-830a-4d28-8707-86d613a39e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code to answer the question above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dfff15",
   "metadata": {},
   "source": [
    "#### 7a.ii. What are the 15 most common words from df_nonmonetarynews?\n",
    "\n",
    "Answer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f303a094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code to answer the question above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4430b7d7",
   "metadata": {},
   "source": [
    "#### 7a.iii. What differences do you notice?\n",
    "\n",
    "Answer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6ce7de-c88e-4c0b-903a-187d2ff6aad7",
   "metadata": {},
   "source": [
    "#### 7b. Visualize the word use in the different types of articles using a word cloud. Below is the code for making the word cloud for the df_news dataframe. You must adapt it to the other dataframes: \n",
    "```python\n",
    "from wordcloud import WordCloud\n",
    "allwords = ' '.join(words)\n",
    "word_cloud = WordCloud(collocations=False, background_color='white').generate(allwords)\n",
    "plt.imshow(word_cloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title('Word Cloud for US Economics Articles')\n",
    "plt.show()\n",
    "``` \n",
    "    \n",
    "i. What is the word cloud for df_monetarynews? (3 points)\n",
    "\n",
    "ii. What is the word cloud df_nonmonetarynews? (3 points)      \n",
    "\n",
    "iii. What differences do you notice? Do these differences seem consistent with your list of top 15 most common words? (1 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d2c9ff",
   "metadata": {},
   "source": [
    "#### 7b.i. What is the word cloud for df_monetarynews?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71e3656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code to answer the question above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb97182",
   "metadata": {},
   "source": [
    "#### 7b.ii. What is the word cloud for df_nonmonetarynews?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d848aaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code to answer the question above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41ea7bb",
   "metadata": {},
   "source": [
    "#### 7b.iii. What differences do you notice? Do these differences seem consistent with your list of top 15 most common words?\n",
    "\n",
    "Answer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce5b7a9-bc8f-4c37-82cb-5523bb0c9ad6",
   "metadata": {},
   "source": [
    "### 8. Monetary Uncertainty in the News: \n",
    "\n",
    "Loughran and McDonald (2011) have created a commonly used bank of word-sentiment lists. One list is a list of “uncertainty words.” You can find this dataset in the Github HW1 Repository. \n",
    "\n",
    "The following is code to make a Monetary Uncertain Score from df_monetarynews and to plot the figure over time. However, there are three things wrong with in the code.\n",
    "\n",
    "#### 8a. Identify the typos, run the correct code, and insert the graph below. \n",
    "\n",
    "HINT: Run the code line by line and manually view the objects that were created and/or the error codes that appear.\n",
    "\n",
    "Answer: The three typos are...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6900bbc-6f3a-4b10-baf2-c68578211563",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'TYPE FILE PATH TO WHERE YOU HAVE THE DATA'\n",
    "\n",
    "# Word Lists\n",
    "uncertainty_wordlist_LM = pd.read_csv(os.path.join(folder_path,\"LM_Uncertainty.csv\"), encoding='utf-8')\n",
    "uncertainty_wordlist_LM = uncertainty_wordlist_LM['uncertain words'].tolist()\n",
    "\n",
    "# Text Uncertainty Score for Each Article\n",
    "uncertainty_score = []\n",
    "for row in df_monetarynews['tokens']:\n",
    "    u_tally = 0\n",
    "    for word in uncertainty_wordlist_LM:\n",
    "        if word in row:\n",
    "            u_tally += 1\n",
    "uncertainty_score.append(u_tally)\n",
    "                \n",
    "df_monetarynews['text_uncertainty_score'] = uncertainty_score     \n",
    "    \n",
    "# Plot Yearly Mean Monetary Policy Uncertainty Over Time\n",
    "\n",
    "#Take mean over years\n",
    "df_monetarynews['year'] = df_monetarynews['date'].dt.year.astype(str)\n",
    "df_monetarynews_yearly = df_monetarynews.groupby(by=['year']).average()\n",
    "\n",
    "df_monetarynews_yearly['year'] = pd.to_datetime(df_monetarynews_yearly.index)\n",
    "\n",
    "#Plot time Series\n",
    "plt.plot(df_monetarynews_yearly['year'],df_monetarynews['text_uncertainty_score'])\n",
    "plt.xlabel('Years')\n",
    "plt.ylabel('Mean Uncertainty Score, Yearly')\n",
    "plt.title('Uncertainty in US Monetary Policy News Articles')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
